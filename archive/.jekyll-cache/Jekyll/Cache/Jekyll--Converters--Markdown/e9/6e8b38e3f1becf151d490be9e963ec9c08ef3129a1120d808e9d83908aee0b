I"oÓ<p>As I have gone through a couple rounds of interviews for data scientist
positions, I‚Äôve been compiling notes on what I consider to be the essential
areas of knowledge. I want to make these notes available to the general public;
although there are many blog posts out there that are supposed to help one
prepare for data science interviews, I haven‚Äôt found any of them to be very
high-quality.</p>

<p>From my perspective, there are four key subject areas that a data scientist
should feel comfortable with when going into an interview:</p>

<ol>
  <li>Statistics (including experimental design)</li>
  <li>Machine Learning</li>
  <li>Software Engineering (including SQL)</li>
  <li>‚ÄúSoft‚Äù Questions</li>
</ol>

<p>I‚Äôm going to go through each of these individually. This first post will focus
on statistics. We will go over a number of topics in statistics in no particular
order. Note that <strong>this post will not teach you statistics; it will remind you
of what you should already know.</strong></p>

<p>If you‚Äôre utterly unfamiliar with the concepts I‚Äôm mentioning, I‚Äôd recommend <a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/index.htm">this
excellent MIT course on probability &amp; statistics</a> as a good starting point. When I
began interviewing, I had never taken a statistics class before; I worked through the
notes, homeworks, and exams for this course, and at the end had a solid foundation to
learn the specific things that you need to know for these interviews. In my studying, I
also frequently use <a href="https://stats.stackexchange.com">cross-validated</a>, a website for asking and answering questions
about statistics. It‚Äôs good for in-depth discussions of subtle issues in
statistics. Finally, <a href="https://www.goodreads.com/book/show/619590.Bayesian_Data_Analysis">Gelman‚Äôs book</a> is the classic in Bayesian inference. If you
have recommendations for good books that cover frequentist statistics in a clear manner,
I‚Äôd love to hear them.</p>

<p>These are the notes that I put together in my studying, and I‚Äôm sure that there is
plenty of room for additions and corrections. I hope to improve this guide over time;
please let me know in the comments if there‚Äôs something you think should be added,
removed, or changed!</p>

<h1 id="the-central-limit-theorem">The Central Limit Theorem</h1>

<p>The Central Limit Theorem is a fundamental tool in statistical analysis. It states
(roughly) that when you add up a bunch of independent and identically distributed random
variables (with finite variance) then their sum will converge to a Gaussian
distribution.<sup id="fnref:fnote1" role="doc-noteref"><a href="#fn:fnote1" class="footnote" rel="footnote">1</a></sup></p>

<p>How is this idea useful to a data scientist? Well, one place where we see a sum of
random variables is in a <em>sample mean</em>. One consequence of the central limit theorem is
that the sample mean of a variable with mean \(\mu\) and variance \(\sigma^2\) will
itself have mean \(\mu\) and variance \(\sigma^2/n\), where \(n\) is the number of
samples.</p>

<p>I‚Äôd like to point out that this is pretty surprising. The distribution of the sum of two
random variables is not, in general, trivial to calculate. So it‚Äôs kind of awesome that,
if we‚Äôre adding up a large enough number of (independent and identically distributed)
random variables, then we <em>do</em>, in fact, have a very easy expression for the
(approximate) distribution of the sum. Even better, we don‚Äôt need to know much of
anything about the distribution of we‚Äôre sampling from, besides its mean and
variance - it‚Äôs other moments, or general shape, don‚Äôt matter for the CLT.</p>

<p>As we will see below, the simplification that the CLT introduces is the basis of one of
the fundamental hypothesis tests that data scientists perform: testing equality of
sample means. For now, let‚Äôs work through an example of the theorem itself.</p>

<h2 id="an-example">An Example</h2>

<p>Suppose that we are sampling a Bernoulli random variable. This is a 0/1 random
variable that is 1 with probability \(p\) and 0 with probability \(1-p\). If we
get the sequence of ten draws \([0,1,1,0,0,0,1,0,1,0]\), then our sample mean is</p>

\[\hat \mu = \frac{1}{10}\sum_{i=1}^{10} x_i = 0.4\]

<p>Of course, this sample mean is itself a random variable - when we report it, we
would like to report an estimate on its variance as well. The central limit
theorem tells us that this will, as \(n\) increases, converge to a Gaussian
distribution. Since the mean of the Bernoulli random variable is \(p\) and its
variance is \(p(1-p)\), we know that the distribution of the sample mean will
converge to a Gaussian with mean \(p\) and variance \(p(1-p)/n\). So we could
say that our estimate of the parameter \(p\) is 0.4 \(\pm\) 0.155. Of course,
we‚Äôre playing a bit loose here, since we‚Äôre using the estimate \(\hat p\) from
the data, as we don‚Äôt actually know the <em>true</em> parameter \(p\).</p>

<p>Now, a sample size of \(n=10\) is a bit small to be relying on a ‚Äúlarge-\(n\)‚Äù
result like the CLT. Actually, in this case, we know the exact distribution of
the sample mean, since \(\sum_i x_i\) is binomially distributed with parameters
\(p\) and \(n\).</p>

<h2 id="other-questions-on-the-clt">Other Questions on the CLT</h2>

<p>I find that the CLT more comes up as a piece of context in other questions
rather than as something that gets asked about directly, but you should be
prepared to answer the following questions.</p>

<ul>
  <li>
    <p><strong>What is the central limit theorem?</strong> We‚Äôve addressed this above - I doubt
they‚Äôll be expecting a mathematically-correct statement of the theorem, but
you should know the gist of it, along with significant limitations (finite
variance being the major one).</p>
  </li>
  <li>
    <p><strong>When can you <em>not</em> use the CLT?</strong> I think the key thing here is that you
have to be normalizing the data in an appropriate way (dividing by the sample
size), and that the underlying variance must be finite. The answer here can
get very subtle and mathematical, involving modes of convergence for random
variables and all that, but I doubt they will push you to go there, unless
you‚Äôre applying for a job specifically as a statistician.</p>
  </li>
  <li>
    <p><strong>Give me an example of the CLT in use.</strong> The classic example here is the
distribution of the sample mean converging to a normal distribution as the
number of samples grows large.</p>
  </li>
</ul>

<h1 id="hypothesis-testing">Hypothesis Testing</h1>

<p>Hypothesis testing (also known by the more verbose ‚Äúnull hypothesis significance
testing‚Äù) is a huge subject, both in scope and importance. We use statistics to
quantitatively answer questions based on data, and (for better or for worse) null
hypothesis significance testing is one of the primary methods by which we construct
these answers.</p>

<p>I won‚Äôt cover the background of NHST here. It‚Äôs well-covered in the MIT course; look at
<a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/">the readings</a> to find the relevant sections. Instead of covering the background,
we‚Äôll work through one exampleof a hypothesis test. It‚Äôs simple, but it comes up all the
time in practice, so it‚Äôs essential to know. I might go so far as to say that this is
the fundamental example of hypothesis testing in data science.</p>

<h2 id="an-example-1">An Example</h2>

<p>Suppose we have two buttons, one green and one blue. We put them in front of
two different samples of users. For simplicity, let‚Äôs say that each sample has
size \(n=100\). We observe that \(k_\text{green}\) 57 users click the green
button, and only \(k_\text{blue} = 48\) click the blue button.</p>

<p>Seems like the green button is better, right? Well, we want to be able to say
how <em>confident</em> we are of this fact. We‚Äôll do this in the language of null
hypothesis significance testing. As you should (hopefully) know, in order to do NHST, we
need a null hypothesis and a test statistic; we need to know the test statistic‚Äôs
distribution (under the null hypothesis); and we need to know the probability of
observing a value ‚Äúat least as extreme‚Äù as the observed value according to this
distribution.</p>

<p>I‚Äôm going to lay out a table of all the important factors here, and then discuss how we
use them to arrive at our \(p\)-value.</p>

<table>
  <thead>
    <tr>
      <th>Description</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Null Hypothesis</td>
      <td>\(p_{blue} - p_{green} &lt; 0\)</td>
    </tr>
    <tr>
      <td>Test Statistic</td>
      <td>\(\frac{k_\text{blue}}{n} - \frac{k_\text{green}}{n}\)</td>
    </tr>
    <tr>
      <td>Test Statistic‚Äôs Distribution</td>
      <td>\(N(0, (p_b(1-p_b) + p_g(1-p_g)) / n)\)</td>
    </tr>
    <tr>
      <td>Test Statistic‚Äôs Observed Value</td>
      <td>-0.09</td>
    </tr>
    <tr>
      <td>\(p\)-value</td>
      <td>0.1003</td>
    </tr>
  </tbody>
</table>

<p>There are a few noteworthy things here. First, we really want to know whether
\(p_g &gt; p_b\), but that‚Äôs equivalent to \(p_b-p_g &lt; 0\). Second, we assume that
\(n\) is large enough so that \(k/n\) is approximately normally distributed,
with mean \(\mu = p\) and variance \(\sigma^2 = p(1-p)/n\). Third, since the
differences of two normals is itself a normal, the test statistic‚Äôs distribution
is (under the null hypothesis) a normal with mean zero and the variance given
(which is the sum of the two variances of \(k_b/n\) and \(k_g/n\)).</p>

<p>Finally, we don‚Äôt actually know \(p_b\) or \(p_g\), so we can‚Äôt really compute
the \(p\)-value; what we do is we say that \(k_b/n\) is ‚Äúclose enough‚Äù‚Äù to
\(p_b\) and use it as an approximation. That gives us our final \(p\)-value.</p>

<p>The \(p\)-value was calculated in Python, as follows:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="n">pb</span> <span class="o">=</span> <span class="mf">0.48</span>
<span class="n">pg</span> <span class="o">=</span> <span class="mf">0.57</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">pb</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">pb</span><span class="p">)</span> <span class="o">+</span> <span class="n">pg</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">pg</span><span class="p">))</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>
<span class="n">norm</span><span class="p">.</span><span class="n">cdf</span><span class="p">(</span><span class="o">-</span><span class="mf">0.09</span><span class="p">,</span> <span class="n">loc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">)</span> <span class="c1"># 0.10034431272089045</span></code></pre></figure>

<p>Calculating the CDF of a normal at \(x=-0.09\) tells us the probability that the test
statistic is less than or equal to \(-0.09\), which is to say the probability that our
test statistic is at least as extreme as the observed value. This probability is
precisely our \(p\)-value.</p>

<p>So what‚Äôs the conclusion? Well, often times a significance level is set before the test
is performed; if the \(p\)-value is not below this threshold, then the null hypothesis
is not rejected. Suppose we had set a significance level of 0.05 before the test began -
then, with this data, we would not be able to reject the null hypothesis, which is that
the buttons are equally appealing to users.</p>

<p>Phew! I went through that pretty quick, but if you can‚Äôt follow the gist of what
I was doing there, I‚Äôd recommend you think through it until it is clear to
you. You will be faced with more complicated situations in practice; it‚Äôs
important that you begin by understanding the most simple situation inside out.</p>

<h2 id="other-topics-in-hypothesis-testing">Other Topics in Hypothesis Testing</h2>

<p>Some important follow-up questions you should be able to answer:</p>

<ul>
  <li>
    <p><strong>What are Type I &amp; II error? What is a situation where you would be more concerned
with Type I error? Vice versa?</strong> These are discussed <a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error">on Wikipedia</a>. Type I error
is false-positive error. You might be very concerned with Type I error if you are
interviewing job candidates; it is very costly to hire the wrong person for the job,
so you really want to avoid false positives. Type II error is false-negative error. If
you are testing for a disease that is deadly but has a simple cure, then you would
certainly NOT want to have a false negative result of the test, since that would
result in an easily-avoidable negative outcome.</p>
  </li>
  <li>
    <p><strong>What is the <em>power</em> of a test? How do you calculate it?</strong> The power of a test is the
probability that you will reject the null hypothesis, given an alternative
hypothesis. Therefore, to calculate the power, you need an alternative hypothesis; in
the example above, this would look like \(p_b-p_g = -0.1\). Although these alternative
hypothesis are often somewhat ad-hoc, the power analysis depends critically upon
them. Google will turn up plenty of videos and tutorials on calculating the power of a
test.</p>
  </li>
  <li>
    <p><strong>What is the significance of a test?</strong> This is the same as the
\(p\)-value threshold below which we reject the null
hypothesis. (In)famously, 0.05 has become the de-facto standard throughout
many sciences for significance levels worthy of publication.</p>
  </li>
  <li>
    <p><strong>Gow would you explain a p-value to a lay person</strong>? Of course, you should
have a solid understanding of the statistical definition of the
\(p\)-value. A generally accepted answer is ‚Äúa \(p\)-value quantifies the
evidence for a hypothesis - closer to zero means more evidence.‚Äù Of course,
this is wrong on a lot of levels - it‚Äôs actually quantifying evidence
<em>against</em> the null hypothesis, not <em>for</em> the alternative. For what it‚Äôs
worth, I‚Äôm not convinced there‚Äôs a great answer to that one; it‚Äôs an
inherently technical quantity that is frequently misrepresented and abused by
people trying to (falsely) simplify its meaning.</p>
  </li>
  <li>
    <p><strong>If you measure many different test statistics, and get a \(p\)-value for each (all
based on the same null hypothesis), how do you combine them to get an aggregate
\(p\)-value?</strong> This one is more of a bonus question, but it‚Äôs worth knowing. It‚Äôs
actually not obvious how do to this, and the true \(p\)-value depends on how the tests
depend on each other. However, you can get an upper-bound (worst-case estimate) on the
aggregate \(p\)-value by adding together the different \(p\)-values. The validity of
this bound results from the inclusion-exclusion principle.</p>
  </li>
</ul>

<h1 id="confidence-intervals">Confidence Intervals</h1>

<p>Confidence intervals allow us to state a statistical result as a range, rather than a
single value. If we count that 150 out of 400 people sample randomly from a city
identify themselves as male, then our best estimate of the fraction of women in the city
is 250/400, or 5/8. But we only looked at 400 people, so it‚Äôs reasonable to expect that
the true value might be a bit more or less than 5/8. Confidence intervals allow us to
quantify this width in a statistically rigorous way.</p>

<p>As per usual, we won‚Äôt actually introduce the concepts here - I‚Äôll refer you to the
<a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/">readings from the MIT course</a> for an introduction. We‚Äôll focus on working through
an example, and looking at some different approaches.</p>

<h2 id="the-exact-method">The Exact Method</h2>

<p>Suppose that we want to find a 95% confidence inverval on the female fraction in the
city discussed above. This corresponds to a significance level of \(\alpha/2\). One way
to get the <strong>exact confidence inverval</strong> is to use the CDF of our test statistic, but
substitute in the observed parameter for the true parameter, and then invert it to find
where it hits \(\alpha/2\) and \(1-\alpha/2\). That is, we need to find the value
\(p_l\) that solves the equation</p>

\[CDF\left(n, p_l\right) = \alpha/2\]

<p>and the value \(p_u\) that solves the equation</p>

\[CDF\left(n, p_u\right) = 1 - \alpha/2.\]

<p>In these, \(CDF(n,p)\) is the cumulative distribution function of our test statistic,
assuming that the true value of \(p\) is in fact the observed value \(\hat p\). This is
a bit confusing, so it‚Äôs worth clarifying. In our case, the sample statistic is the
sample mean of \(n\) binomial random variables, so this CDF is the CDF of the sample
mean of \(n\) binomial random variables with parameter \(5/8\). Solving the two
equations above would give us our confidence inverval \([p_l, p_u]\).</p>

<p>It took me a bit of work to see that solving the above two equations would in fact give
us bounds that satisfy the definitions of a \(1-\alpha\) confidence interval, which says
that, were we to run many experiments, we would find that the true value of \(p\) would
fall between \(p_l\) and \(p_u\) with the probability</p>

\[P\left(p_l\leq p \leq p_u\right) = 1-\alpha.\]

<p>If you‚Äôre into this sort of thing, I‚Äôd suggest you take some time thinking through why
inverting the CDF as above guarantees bounds \([p_l, p_u]\) that solve the above
equaiton.</p>

<p>Although it is useful for theoretical analysis, I rarely use this method in
practice, because I often do not actually know the true CDF of the statistic
I am measuring. Sometimes I do know the true CDF, but even in such cases, the
next (approximate) method is generally sufficient.</p>

<h2 id="the-approximate-method">The Approximate Method</h2>

<p>If your statistic can be phrased as a sum, then its distribution approaches a normal
distribution.<sup id="fnref:fnote2" role="doc-noteref"><a href="#fn:fnote2" class="footnote" rel="footnote">2</a></sup> This means that you can solve the above equations for a normal
CDF rather than the true CDF of the sum (in the case above, a binomial CDF).</p>

<p>How does this help? For a normal distribution, the solutions for the above equations to
find lower and upper bounds are well known. In particular, the inverval
\([\mu-\sigma,\mu+\sigma]\), also called a \(1\sigma\)-interval, covers about 68% of the
mass (probability) of the normal PDF, so if we wanted to find a confidence interval of
level \(0.68\), then we know to use the bounds \((\overline x-\sigma, \overline
x+\sigma)\), where \(\overline x\) is our estimate of the true mean \(\mu\).</p>

<p>This sort of result is very powerful, because it saves us from having to do any
inversion by hand. A table below indicates the probability mass contained in various
symmetric intervals on a normal distribution:</p>

<table>
  <thead>
    <tr>
      <th>Inverval</th>
      <th>Width<sup id="fnref:fnote3" role="doc-noteref"><a href="#fn:fnote3" class="footnote" rel="footnote">3</a></sup></th>
      <th>Coverage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\([\mu-\sigma,\mu+\sigma]\)</td>
      <td>\(1\sigma\)</td>
      <td>0.683</td>
    </tr>
    <tr>
      <td>\([\mu-2\sigma,\mu+2\sigma]\)</td>
      <td>\(2\sigma\)</td>
      <td>0.954</td>
    </tr>
    <tr>
      <td>\([\mu-3\sigma,\mu+3\sigma]\)</td>
      <td>\(3\sigma\)</td>
      <td>0.997</td>
    </tr>
  </tbody>
</table>

<p>Let‚Äôs think through how we would use this in the above example, where we give a
confidence interval on our estimate of the binomial parameter \(p\).</p>

<p>A binomial distribution has mean \(\mu=np\) and variance \(\sigma^2=np(1-p)\). Since
the sample statistical \(\hat p\) is just the binomial divided by \(n\), it has mean
\(\mu=p\) and variance \(\sigma^2 = p(1-p)/n\). The central limit theorem tells us that
the distribution of \(\hat p\) will converge to a normal with just these parameters.</p>

<p>Suppose we want an (approximate) 95% confidence interval on the percentage of women in
the population of our city; the table above tells us we can just do a two-sigma
interval. (This is not <em>exactly</em> a 95% confidence interval; it‚Äôs a bit over, as we see
in the table above). The parameter \(\hat p\) has mean \(\mu= p\) and variance
\(\sigma^2 = p(1-p)/n\).<sup id="fnref:fnote4" role="doc-noteref"><a href="#fn:fnote4" class="footnote" rel="footnote">4</a></sup> In our case, \(\hat p=5/8\), so our confidence
interval is \(5/8 \pm 15/1280 \approx 0.625 \pm 0.0117\). Note that we approximated
\(p\) with our experimental value \(\hat p\); the theoretical framework that allows us
to do this substitution is beyond the scope of this article, but is nicely covered in
the MIT readings (Reading 22, in particular).</p>

<h2 id="the-bootstrap-method">The Bootstrap Method</h2>

<p>The previous approach relies on the accuracy of approximating our statistic‚Äôs
distribution by a normal distribution. Bootstrapping is a pragmatic, flexible
approach to calculating confidence intervals, which makes no assumptions on the
underlying statistics we are calculating. We‚Äôll go into more detail on
bootstrapping in general below, so we‚Äôll be pretty brief here.</p>

<p>The basic idea is to repeatedly pull 400 samples <em>with replacement</em> from the sampled
data. For each set of 400 samples, we get an estimate \(\hat p\), and thus can build an
empirical distribution on \(\hat p\). Of course, the CLT indicates that this empirical
distribution should look a lot like a gaussian distribution with mean \(\mu= p\) and variance
\(\sigma^2 = p(1-p)/n\)..</p>

<p>Once you have bootstrapped an empirical distribution for your statistic of interest (in
the example above, this is the percentage of the population that is women), then you can
simply find the \(\alpha/2\) and \(1-\alpha/2\) percentiles, which then become your
confidence interval. Although in this case our empirical distribution is (approximately)
normal, it‚Äôs worth realizing that we can reasonably calculate percentiles <em>regardless</em>
of what the empirical distribution is; this is why bootstrapping confidence intervals
are so flexible.</p>

<p>As you‚Äôll see below, the downside of bootstrapping confidence intervals is that
it requires some computation. The amount of computation required can be
anywhere from trivial to daunting, depending on how many samples you want in
your empirical distribution. Another downside is that their statistical interpretation
is not exactly in alignment with the definition of a confidence interval, but I‚Äôll leave
the consideration of that as an exercise for the reader.<sup id="fnref:fnotez" role="doc-noteref"><a href="#fn:fnotez" class="footnote" rel="footnote">5</a></sup> <a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf">One of the MIT
readings</a> has an in-dpeth discussion of confidence intervals generated via the
bootstrap method.</p>

<p><strong>Overall, I would recommend using the approximate method when you have good reason to
believe your sample statistic is approximately normal, or bootstrapping otherwise.</strong> Of
course, the central limit theorem can provide some guarantees about the asympototic
distribution of certain statistics, so it‚Äôs worth thinking through whether that applies
to your situations.</p>

<h2 id="other-topics-in-confidence-intervals">Other Topics in Confidence Intervals</h2>

<ul>
  <li>
    <p><strong>What is the definition of a confidence interval?</strong> This is a bit more technical, but
it‚Äôs essential to know that it is <strong>not</strong> ‚Äúthere is a 95% probability that the true
parameter is in this range.‚Äù Actually, what it means is that ‚Äúif you reran the
experiment many times, then 95% of the time, the true value of the parameter you‚Äôre
estimating would fall in this range.‚Äù It‚Äôs worth noting that the <em>range</em> is the random
variable here - the parameter itself (the true percentage of the population that
identifies as female, in our example) is fixed.</p>
  </li>
  <li>
    <p><strong>How would this change if you wanted a <em>one-sided</em> confidence interval?</strong>
This one isn‚Äôt too bad - you just solve either \(CDF(n,p_l) = \alpha\) or
\(CDF(n,p_u) = 1-\alpha\) for a lower- or upper-bounded interval,
respectively.</p>
  </li>
  <li>
    <p><strong>What is the relationship between confidence intervals and hypothesis testing?</strong>
There are many ways to answer this question; it‚Äôs a good one to ponder in order to get
a deeper understanding of the two topics. One connection is the relationship between
confidence intervals and rejection regions in NHST - Reading 22 in the MIT course
addresses this one nicely.</p>
  </li>
</ul>

<h1 id="bootstrapping">Bootstrapping</h1>

<p>Bootstrapping is a technique that allows you to get insight into the quality of your
estimates, based only on the data you have. It‚Äôs a key tool in a data scientist‚Äôs
toolbag, because we frequently don‚Äôt have a clear theoretical understanding of our
statistics, and yet we want to provide uncertainty estimates. To understand how it
works, let‚Äôs look through an example.</p>

<p>In the last section, we sampled 400 people in an effort to understand what percentage of
a city‚Äôs population identified as female. Since 250 of them identified themselves as
female, our estimate of the raio for the total population is \(5/8\). This estimate it
itself a random variable; if we had sampled different people, we might have ended up
with a different number. What if we want to know the distribution of this estimate? How
would we go about getting that?</p>

<p>Well, the obvious way is to go out and sample 400 more people, and repeat this over and
over again, until we have many such fractional estimates. But what if we don‚Äôt have
access to sampling more people? The natural thing is to think that we‚Äôre out of luck -
without the ability to sample further, we can‚Äôt actually understand more about the
distribution of our parameter (ignoring, for the moment, that we have lots of
theoretical knowledge about it via the CLT).</p>

<p>The idea behind bootstrapping is simple. Sample from the data you already have, with
replacement, a new sample of 400 people. This will give you an estimate of the female
fraction that is distinct from your original estimate, due to the replacement in your
sampling. You can repeat this process as many times as you like; you will then get an
empirical distribution whic approaches the true distribution of the statistic.<sup id="fnref:fnote4:1" role="doc-noteref"><a href="#fn:fnote4" class="footnote" rel="footnote">4</a></sup></p>

<p>Bootstrapping has the advantage of belig flexible, although it does have its
limitations. Rather than get too far into the weeds, I‚Äôll just point you to the
<a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">Wikipedia article on bootstrapping</a>. There are also tons of resources about this
subject online. Try coding it up for yourself! By the time you‚Äôre interviewing, you
should be able to write a bootstrapping algorithm quite easily.</p>

<p><a href="https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/">Machine Learning Mastery</a> has a good introduction to bootstrapping that uses the
scikit-learn API. <a href="https://towardsdatascience.com/an-introduction-to-the-bootstrap-method-58bcb51b4d60">Towards Data Science</a> codes it up directly in NumPy, which is a
useful thing to know how to be able to do. Asking someone to code up a bootstrapping
function would be an entirely reasonable interview questions, so it‚Äôs something you
should be comfortable doing.</p>

<h2 id="other-topics-in-bootstrapping">Other Topics in Bootstrapping</h2>

<ul>
  <li><strong>When would you <em>not</em> want to use bootstrapping?</strong> It might not be feasible when it
is very costly to calculate your sample statistic. To get accurate estimates you‚Äôll
need to calculate your statistic thousands of times, so it might not be feasible if it
takes minutes or hours to calculate a single sample. Also, it is often difficult to
get strong theoretical guarantees about probabilities based on bootstrapping, so if
you need a highly statistically rigorous approach, you might be better served with
something more analytical. Finally, if you know the distribution of your statistic
already (for example, you know from the CLT that it is normally distributed) then you
can get better (more accurate) uncertainty estimates from an analytical approach.</li>
</ul>

<h1 id="linear-regression">Linear Regression</h1>

<p>Regression is the study of the relationship between variables; for example, we
might wish to know how the weight of a person relates to their height. <em>Linear</em>
regression assumes that your input (height, or \(h\)) and output (weight, or
\(w\)) variables are <em>linearly related</em>, with slope \(\beta_1\), intercept
\(\beta_0\), and noise \(\epsilon\).</p>

\[w = \beta_1\cdot h + \beta_0 + \epsilon.\]

<p>A linear regression analysis helps the user discover the \(\beta\)s in the
above equation. This is just the simplest application of LR; in reality, it is
quite flexible and can be used in a number of scenarios.</p>

<p>Linear regression is another large topic that I can‚Äôt really do justice to in this
article. Instead, I‚Äôll just go through some of the common topics, and introduce the
questions you should be able to address. As is the case with most of these topics, you
can look at the <a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/index.htm">MIT Statistics &amp; Probability course</a> for a solid academic
introduction to the subject. You can also dig through <a href="https://en.wikipedia.org/wiki/Linear_regression">the Wikipedia article</a> to get
a more in-depth picture. The subject is so huge, and there‚Äôs so much to learn about it,
that you really can spend as much time as you want digging into it - I‚Äôm just going to
gesture at some of the simpler aspects of it.</p>

<h2 id="calculating-a-linear-regression">Calculating a Linear Regression</h2>

<p>Rather than go through an example here, I‚Äôll just refer you to the many available guides
that show you how to do this in code. Of course, you could do it in raw NumPy, solving
the normal equations explicitly, but I‚Äôd recommend using scikit-learn or statsmodels, as
they have much nicer interfaces, and give you all sorts of additional information about
your model (\(r^2\), \(p\)-value, etc.)</p>

<p><a href="https://realpython.com/linear-regression-in-python/">Real Python</a> has a good guide to coding this up - see the section ‚ÄúSimple Linear
Regression with scikit-learn.‚Äù <a href="https://www.geeksforgeeks.org/linear-regression-python-implementation/">GeeksForGeeks</a> does the solution in raw NumPy; the
equations won‚Äôt be meaningful for you until you read up on the normal equation and how
to analytically solve for the optimal LR coefficients. If you want something similar in
R, or Julia, or MATLAB,<sup id="fnref:fnoted" role="doc-noteref"><a href="#fn:fnoted" class="footnote" rel="footnote">6</a></sup> then I‚Äôm sure it‚Äôs out there, you‚Äôll just have to go do
some Googling to find it.</p>

<h2 id="a-statistical-view">A Statistical View</h2>

<p>This subject straddles the boundary between statistics and machine-learning. It has been
quite thoroughly studied from a statistical point of view, and there are some iportant
results that you should be familiar with when thinking about linear regression from a
statistical frame.<sup id="fnref:fnotec" role="doc-noteref"><a href="#fn:fnotec" class="footnote" rel="footnote">7</a></sup></p>

<p>Let‚Äôs look back at our foundational model for linear regression. LR assumes
that your input \(x\) and output \(y\) are related via</p>

\[y_i = \beta_1\cdot x_i + \beta_0 + \epsilon_i,\]

<p>where \(\epsilon_i\) are i.i.d., distributed as \(N(0, \sigma^2)\). Since the
\(\epsilon\) are random variables, the \(\beta_j\) are themselves random
variables. One important question is whether there is, in fact, any
relationship between our variables at all. If there is not, then we should
\(\beta_1\) close to 0,<sup id="fnref:fnoteb" role="doc-noteref"><a href="#fn:fnoteb" class="footnote" rel="footnote">8</a></sup> but they will not ever be exactly zero. One important
statistical technique in LR is <strong>doing a hypothesis test against the null
hypothesis that \(\beta_1 = 0\)</strong>. When a package like scikit-learn returns a
‚Äú\(p\)-value of the regression‚Äù, this is the \(p\)-value they are talking
about.</p>

<p>Like I said before, there is a lot more to know about the statistics of linear
regression than just what I‚Äôve said here. You can learn more about the statistics of LR
by looking at the <a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading25.pdf">MIT course notes on the subject</a>, or by digging through your
favorite undergraduate statistics book - most of them should have sections covering it.</p>

<h2 id="validating-your-model">Validating Your Model</h2>

<p>Once you‚Äôve calculated your LR, you‚Äôd like to validate it. This is very important to
do - if you‚Äôre asked to calculate a linear regression in an interview, you should always
go through the process of validating it after you‚Äôve done the calculation.</p>

<p>I‚Äôd generally go through the following steps:</p>

<ul>
  <li>If it‚Äôs just a simple (one independent variable) linear regression, then plot the two
variables. This should give you a good sense of whether it‚Äôs a good idea to use linear
regression in the first place. If you have multiple independent variables, you can
make separate plots for each one.</li>
  <li>Look at your \(r^2\) value. Is it reasonably large? Remember, closer to 1 is
better. If it‚Äôs small, then doing a linear regression hasn‚Äôt helped much.</li>
  <li>You can look at the \(p\)-value to see if it‚Äôs difference from zero is
statistically significant (see the section below). Also, you can have a very
significant \(p\)-value while still having a low \(r^2\), so be cautious in your
interpretation of this one.</li>
  <li>You can also look at the RMSE of your model, but this number is not scaled between 0
and 1, so a ‚Äúgood‚Äù RMSE is highly dependent on the units of your indepedent variable.</li>
  <li>Plot your residuals, for each variable. The residual is just the input minus
the value predicted by your model, a.k.a. the error of your model. Plotting
each residual isn‚Äôt really feasible if you have hundreds of independent
variables, but it‚Äôs a good idea if your data is small enough. You should be
looking for ‚Äúhomoskedasticity‚Äù - that the variance of the error is uniform
across the range of the independent variable. If it‚Äôs not, then certain
things you‚Äôve calculated (for example, the \(p\)-value of your regression)
are no longer valid. You might also see that your errors have a bias that
changes as the \(x_i\) changes; this means that there‚Äôs some more complicated
relationship between \(y\) and \(x_i\) that your regression did not pick up.</li>
</ul>

<p>Some of the questions below address the assumptions of linear regression; you
should be familiar with them, and now how to test for them either before or
after the regression is performed, so that you can be confident that your model
is valid.</p>

<h2 id="basic-questions-on-lr">Basic Questions on LR</h2>

<p>Hopefully you‚Äôve familiarized yourself with the basic ideas behind linear
regression. Here are some conceptual questions you should be able to answer.</p>

<ul>
  <li>
    <p><strong>How are the \(\beta\)s calculated?</strong> Practically, you let the library
you‚Äôre using take care of this. But behind the scenes, generally it‚Äôs solving
the so-called ‚Äúnormal equations‚Äù, which give you the optimal (highest
\(r^2\)) parameters possible.  You can use gradient descent to approximate
the optimal solution when the design matrix is too large to invert; this is
available via the <code class="language-plaintext highlighter-rouge">SGDRegressor</code> model in scikit-learn.</p>
  </li>
  <li>
    <p><strong>How do you decide if you should use linear regression?</strong> The best case is
when the data is 2- or 3-dimensional; then you can just plot the data and see
if it looks like ‚Äúlinear plus noise‚Äù. However, if you have lots of
independent variables, this isn‚Äôt really an option. In such a case, you
should look perform a linear regression analysis, and then look at the errors
to verify that they look normally distributed and homoskedastic (constant
variance).</p>
  </li>
  <li>
    <p><strong>What does the \(r^2\) value of a regression indicate?</strong> The \(r^2\) value
indicates ‚Äúhow much of the variance of the output data is explained by the
regression.‚Äù That is, your output data \(y\) has some (sample) variance, just
on its own. Once you discover the linear relationship and subtract it off,
then the remaining error \(y - \beta_0 - \beta_1x\) still has some variance,
but hopefully it‚Äôs lower - \(r^2\) is one minus the ratio of the original to
the remaining variance. When \(r^2=1\), then your line is a perfect fit of
the data, and there is no remaining error. It is often used to explain the
‚Äúquality‚Äù of your fit, although this can be a bit treacherous - see
<a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet">Anscombe‚Äôs Quartet</a> for examples of very different situations with the
same \(r^2\) value.</p>
  </li>
  <li>
    <p><strong>What are the assumptions you make when doing a linear regression?</strong> The
Wikipedia article <a href="https://en.wikipedia.org/wiki/Linear_regression#Assumptions">addresses this point</a> quite thoroughly. This is worth
knowing, because you don‚Äôt just want to jump in and blindly do LR; you want
to be sure it‚Äôs actually a reasonable approach.</p>
  </li>
  <li>
    <p><strong>When is it a bad idea to do LR?</strong> When you do linear regression, you‚Äôre assuming a
certain relationship between your variables. Just the parameters and output of your
regression won‚Äôt tell you whether the data really are appropriate for a linear
model. <a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet">Anscombe‚Äôs Quartet</a> is a particularly striking example of how the output of
a linear regression analysis can look similar but in fact the quality of the analysis
can be radically different. Beyond this, it is a bad idea to do LR whenever the
assumptions of LR are violated by the data; see the above bullet for more info there.</p>
  </li>
  <li>
    <p><strong>Can you do linear regression on a nonlinear relationship?</strong> In many cases,
yes. What we need is for the model to be linear in the parameters \(\beta\);
if, for example, you are comparing distance and time for a constantly
accelerating object \(d = 1/2at^2\), and you want to do regression to
discover the acceleration \(a\), then you can just use \(t^2\) as your
independent variable. The model relating \(d\) and \(t^2\) is linear in the
acceleration \(a\), as required.</p>
  </li>
  <li>
    <p><strong>What does the ‚Äúlinear‚Äù in linear regression refer to?</strong> This one might seem
trivial, but it‚Äôs a bit of a trick question; the relationship \(y =
2\log(x)\) might not appear linear, but in fact it can be obtained via a
linear regression, by using \(\log(x)\) as the input variables, rather than
\(x\). Of course, for this to work, you need to know ahead of time that you
want to compare against \(\log(x)\), but this can be discovered via
trial-and-error, to some extent. So the ‚Äúlinear‚Äù <em>does</em>, as you‚Äôd expect,
mean that the relationship between independent and dependent variable is
linear, but you can always <em>change</em> either of them and re-calculate your
regression.</p>
  </li>
</ul>

<h2 id="handling-overfitting">Handling Overfitting</h2>

<p>Overfitting is a very important to understand, and is a fundamental challenge in machine
learning and modeling. I‚Äôm not going to go into great detail on it here; more
information will be presented in the machine learning section of the guide. There are
some techniques for handling it that are particular to LR, which is what I‚Äôll talk about
here.</p>

<p><a href="https://realpython.com/linear-regression-in-python/">RealPython</a> has good images showing examples of over-fitting. You can
handle it by building into your model a ‚Äúpenalty‚Äù on the \(\beta_i\)s; that is,
tell your model ‚ÄúI want low error, <strong>and</strong> I don‚Äôt want large coefficients.**
The balance of these preferences is determined by a parameter, often denoted by
\(\lambda\).</p>

<p>Since you have many \(\beta\)s, in general, you have to combine them in some
fashion. Two such ways to calculate the measure of ‚Äúoverall badness‚Äù (which I‚Äôll call
\(OB\)) are</p>

\[OB = \sqrt{ \beta_1^2 + \beta_2^2 + \ldots + \beta_n^2 }\]

<p>or</p>

\[OB = |\beta_1| + |\beta_2| + \ldots + |\beta_n|.\]

<p>The first will tend to be emphasize outliers; that is, it is more sensitive to
single large \(\beta\)s. The second considers all the \(\beta\)s more
uniformly. If you use the first, it is called ‚Äúridge regression‚Äù, and if you
use the second it is called ‚ÄúLASSO regression.‚Äù</p>

<p>In mathematics, these denote the \(\ell_1\) and \(\ell_2\) norms of the vectors
of \(\beta\)s; you can in theory use \(\ell_p\) norms for any \(p\), even
\(p=0\) (count the number of non-zero \(\beta\)s to get the overall badness) or
\(p=\infty\) (take the largest \(\beta\) as the overall badness). However, in
practice, LASSO and ridge regression are already implemented in common
packages, so it‚Äôs easy to use them right out of the box.</p>

<p>As usual, there is a LOT to learn about how LASSO and ridge regression change your
output, and what kinds of problems they can address (and/or create). I‚Äôd highly
recommend searching around the internet to learn more about them if you aren‚Äôt already
confident in your understanding of how they work.</p>

<h2 id="logistic-regression">Logistic Regression</h2>

<p>Logistic regression is a way of modifying linear regression models to get a
classification model. The statistics of logistic regression are, generally speaking, not
as clean as those of linear regression. It will be covered in the machine learning
section, so we won‚Äôt discuss it here.</p>

<h1 id="bayesian-inference">Bayesian Inference</h1>

<p>Up until now this guide has primarily focused on frequentist topics in
statistics, such as hypothesis testing and the frequentist approach to
confidence intervals. There is an entire world of Bayesian statistical
inference, which differs significantly from the frequentist approach in both
philosophy and technique. I will only touch on the most basic application of
Bayesian reasoning in this guide.</p>

<p>In this section, I will mostly defer to outside sources, who I think speak more
eloquently on the topic than I can. Some companies (such as Google, or so I‚Äôm told) tend
to focus on advanced Bayesian skills in their data science interviews; if you want to
really learn the Bayesian approach, I‚Äôd reccomend <a href="https://www.goodreads.com/book/show/619590.Bayesian_Data_Analysis">Gelman‚Äôs book</a>, which is a
classic in the field.</p>

<h2 id="bayesian-vs-frequentist-statistics">Bayesian vs Frequentist Statistics</h2>

<p>It‚Äôs worth being able to clearly discuss the difference in philosophy and approach
between the two schools of statistics. I particularly like the discussion in the MIT
course notes. They state, more or less, that while the Bayesians like to reason from
Bayes theorem</p>

\[P(H|D) = \frac{ P(D|H)P(H)}{P(D)},\]

<p>the frequentist school thinks that ‚Äúthe probability of the hypothesis‚Äù is a nonsense
concept - it is not a well-founded probablistic value, in the sense that there is no
repeatable experiment you can run in which to gather relative frequency counts and
calculate probabilities. Therefore, the frequentists must reason directly from
\(P(D|H)\), the probability of the data given the hypothesis, which is just the
\(p\)-value. The upside of this is that the probabilistic interpretation of \(P(D|H)\)
is clean and unambiguous; the downside is that it is easy to misunderstand, since what
we really think we want is ‚Äúthe probability that the hypothesis is true.‚Äù</p>

<p>If you want to know more about this, there are endless discussions of it all over the
internet. Like many such dichotomies (emacs vs. vim, overhand vs underhand toilet paper,
etc.) it is generally overblown - a working statistician should be familiar with, and
comfortable using, both frequentist <em>and</em> Bayesian techniques in their analysis.</p>

<h2 id="basics-of-bayes-theorem">Basics of Bayes Theorem</h2>

<p>Bayes theorem tells us how to update our belief in light of new evidence. You
should be comfortably applying Bayes theorem in order to answer basic
probability questions. The classic example is the ‚Äúbase rate fallacy‚Äù:</p>

<p>Consider a routine screening test for a disease. Suppose the frequency of the
disease in the population (base rate) is 0.5%. The test is highly accurate with
a 5% false positive rate and a 10% false negative rate. You take the test and
it comes back positive. What is the probability that you have the disease?</p>

<p>The answer is NOT 0.95, even though the test has a 5% false positive rate. You should be
able to clearly work through this problem, building probability tables and using Bayes
theorem to calculate the final answer. The problem is worked through in the <a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading3.pdf">MIT stats
course readings</a> (see Example 10), so I‚Äôll defer to them for the details.</p>

<h2 id="updating-posteriors--conjugate-priors">Updating Posteriors &amp; Conjugate Priors</h2>

<p>The above approach of calculating out all the probabilites by hand works reasonbly well
when there are only a few possible outcomes in the probability space, but it doesn‚Äôt
scale well to large (discrete) probability spaces, and won‚Äôt work at all in continuous
probability spaces. In such situations, you‚Äôre still fundamentally relying on Bayes
theorem, but the way it is applied looks quite different - you end up using sums and
integrals to calculate the relevant terms.</p>

<p>Again, I‚Äôll defer to the <a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/">MIT stats course readings</a> for the details - readings 12
and 13 are the relevant ones here.</p>

<p>It‚Äôs particularly useful to be familiar with the concept of <strong>conjugate
priors</strong>. In general, updating your priors involves computing an integral,
which as anyone who has taken calculus knows can be a pain in the ass. When
sampling from a distribution and estimating the parameters, there are certain
priors for which the updates based on successive samples work out to be very
simple.</p>

<p>For an example of this, suppose you‚Äôre flipping a biased coin and trying to
figure out the bias. This is equivalent to sampling a binomial distribution and
trying to estimate the parameter \(p\). If your prior is uniform (flat across
the interval \([0,1]\)), then after \(N\) flips, \(k\) of which come up heads,
your posterior probability density on \(p\) will be</p>

\[f(p) \propto p^{k}((1-p)^{N-k}.\]

<p>This is called a <strong>\(\beta\) distribution</strong>. It is kind of magical that we can
calculate this without having to do any integrals - this is because the
\(\beta\) distribution is ‚Äúconjugate to‚Äù the binomial distribution. It‚Äôs
important that we started out with a uniform distribution as our prior - if we
had chosen an arbitrary prior, the algebra might not have worked out as
nicely. In particular, if we start with a non-\(\beta\) prior, then this trick
won‚Äôt work, because our prior will not be conjuage to the binomial distribution.</p>

<p>The other important conjugate pair to know is that of the Gaussian
distribution; it is, in fact, conjuage to itself, so if you estimate the
parameters of a normal distribution, those estimates are themselves normal, and
updating your belief about the parameters based on new draws from the normal
distribution is as simple as doing some algebra.</p>

<p>There are many good resources available online and in textbooks discussing
conjuage priors; <a href="https://en.wikipedia.org/wiki/Conjugate_prior">Wikipedia</a> is a good place to start.</p>

<h1 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h1>

<p>We discussed before the case where you have a bunch of survey data, and want to estimate
the proportion of the population that identifies as female. Statistically speaking,
this proportion is a <em>parameter</em> of the probability distribution over gender identity in
the that geographical region. We‚Äôve intuitively been saying that if we see 250 out of
400 respond that they are female, then our best estimate of the proportion is 5/8. Let‚Äôs
get a little more formal about why exactly this is our best estimate.</p>

<p>First of all, I‚Äôm going to consider a simplified world in which there are only two
genders, male and female. I do this to simplify the statistics, not because it is an
accurate model of the world. In this world, if the <em>true</em> fraction of the population
that identifies as female is 0.6, then there is some non-zero probability that you would
draw a sample of 400 people in which 250 identify as female. We call this the
<em>likelihood</em> of the parameter 0.6. In particular, the binomial distribution tells us
that</p>

\[\mathcal{L}(0.6|n_\text{female}=250) =  {400 \choose 250} \,0.6^{250}\, (1-0.6)^{400-250}\]

<p>Of course, I could calculate this for any parameter in \([0,1]\); if I were very far
from 5/8, however, then this likelihood would be very small.</p>

<p>Now, a natural question to ask is ‚Äúwhich parameter \(p\) would give us the highest
likelihood?‚Äù That is, which parameter best fits our data? That is the
<strong>maximum-likelihood estimate</strong> of the parameter \(p\). The actual calculation of that
maximum involves some calculus and a neat trick involving logarithms, but I‚Äôll refer the
reader <a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading10b.pdf">elsewhere</a> for those details. It‚Äôs worth noting that the MLE is often our
intuitive ‚Äúbest guess‚Äù at the parameter; in this case, as you might anticipate,
\(p=5/8\) maximizes the likelihood of seeing 250 people out of 400 identify as female.</p>

<p>I won‚Äôt give any question here, because I honestly have not seen any in my searching
around. Even so, I think it‚Äôs an important concept to be familiar with. Maximum
likelihood estimation often provides a theoretical foundation for our intuitive
estimates of parameters, and it‚Äôs helpful to be able to justify yourself in this
framework.</p>

<p>For example, if you‚Äôre looking at samples from an exponential distribution, and you want
to identify the parameter \(\lambda\), you might guess that since the mean of an
exponential random variable is \(\mu= 1/\lambda\), a good guess would be \(\lambda
\approx 1/\overline x\), where \(\overline x\) is your sample mean. In fact you would be
correct, and this is the MLE for \(\lambda\); you should be familiar with this way of
thinking about parameter estimation.</p>

<h1 id="experimental-design">Experimental Design</h1>

<p>Last, but certainly not least, is the large subject of experimental design. This is a
more nebulous topic, and therefore harder to familiarize yourself with quickly, than the
others we‚Äôve discussed so far.</p>

<p>If we have some new feature, we might have reason to think it will be good to include in
our product. For example, Facebook rolled out a ‚Äústories‚Äù feature some time ago (I
honestly couldn‚Äôt tell you what it does, but it‚Äôs some thing that sits on the top of
your newsfeed). However, before they expose this to all their users, they want to put it
out there ‚Äúin the wild‚Äù and see how it performs. So, they run an experiment.</p>

<p>Designing this experiment in a valid way is essential to getting meaningful, informative
results. An interview question at Facebook might be: <strong>How will you analyze if launching
stories is a good idea? What data would you look at?</strong> The discussion of this question
could easily fill a full 45-minute interview session, as there are many nuances and
details to examine.</p>

<p>One basic approach would be to randomly show the ‚Äústories‚Äù feature to some people, and
not to others, and then see how it affects their behavior. This is an A/B test. Some
questions you should be thinking about are:</p>

<ul>
  <li><strong>What metrics will we want to track in order ot measure the effect of stories?</strong> For
example, we might measure the time spent on the site, the number of clicks, etc.</li>
  <li><strong>How should we randomize the two groups?</strong> Should we randomly choose every time someone
visits the site whether to show them stories or not? Or should we make a choice for
each <em>user</em> and fix that choice? Generally, user-based randomization is preferable,
although sometimes it‚Äôs hard to do across devices (think about why this is).</li>
  <li><strong>How long should we run the tests? How many people should be in each group?</strong> This
decision is often based on a <em>power calculation</em>, which gives us the probability of
rejecting the null hypothesis, given some alternative hypothesis. I personally am not
a huge fan of these because the alternative hypothesis is usually quite ad-hoc, but it
is the standard, so it‚Äôs good to know how to do it. For example, you might demand that
your test be large enough that if including stories increases site visit time by at
least one minute, our A/B test will detect that with 90% probability.</li>
  <li><strong>When can we stop the test?</strong> The important thing to note here is that you <strong>cannot</strong>
just stop the test once the results look good - you have to decide beforehand how long
you want it to run.</li>
  <li><strong>How will you deal with confounding variables?</strong> What if, due to some techincal
difficulty, you end up mostly showing stories to users at a certain time of day, or in
a certain geographical region? There are a variety of approaches here, and I won‚Äôt get
into the details, but it‚Äôs essential that you be able to answer this concern clearly
and thoroughly.</li>
</ul>

<p>It‚Äôs also worth considering scenarios where you have to analyze data after the fact in
order to perform ‚Äúexperiments‚Äù; sometimes you want to know (for example) if the color of
a product has affected how well it sold, and you want to do so using existing sales
data. What limitations might this impose? A key limitation is that of confounding
variables - perhaps the product in red mostly sold in certain geographic regions,
whereas the blue version sold better in other geographic regions. What impact will this
have on your analysis?</p>

<p>There are many other considerations to think about around experimental design. I don‚Äôt
have any particular posts that I like; I‚Äôd recommend searching around Google to find
more information on the topic.</p>

<p>If you have any friends that do statistics professionally, I‚Äôd suggest sketching our a
design for the above experiment and talking through it with them - the ability to think
through an experimental design is something that is best developed over years of
professional experience.</p>

<h1 id="conclusion">Conclusion</h1>

<p>This guide has focused on some of the basic aspects of statistics that get covered in
data science interviews. It is far from exhaustive - different companies focus on
different skills, and will therefore be asking you about different statistical concepts
and techniques. I haven‚Äôt discussed time-dependent statistics at all - Markov chains,
time-series analysis, forecasting, and stochastic processes all might be of interest to
employers if they are relevant to the field of work.</p>

<p>Please let me know if you have any corrections to what I‚Äôve said here. I‚Äôm far
from a statistician, so I‚Äôm sure that I‚Äôve made lots of small (and some large)
mistakes!</p>

<p>Stay tuned for the rest of the study guide, which should be appearing in the
coming months. And finally, best of luck with your job search! It can be a
challenging, and even demoralizing experience; just keep learning, and don‚Äôt
let rejection get you down. Happy hunting!</p>

<!-------------------------------- FOOTER ---------------------------->

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:fnote1" role="doc-endnote">
      <p>Of course, the actual statement is careful about the mode of
convergence, and the fact that it is actually an appropriately-normalized
version of the distribution that converges, and so on.¬†<a href="#fnref:fnote1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fnote2" role="doc-endnote">
      <p>Again, we‚Äôre being loose here - it has to have finite variance, and
the convergence is only in a specific sense.¬†<a href="#fnref:fnote2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fnote3" role="doc-endnote">
      <p>I‚Äôm being a little loose with definitions here - the width of a
\(2\sigma\) inverval is actually \(4\sigma\), but I think most would still
describe it using the phrase ‚Äútwo-sigma‚Äù.¬†<a href="#fnref:fnote3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fnote4" role="doc-endnote">
      <p>As usual, we‚Äôre being a bit sloppy - we‚Äôre just using the sample variance in
place of the true variance and pretending this is correct. This will work if the
number of samples \(n\) is large. If you need confidence intervals with few (say,
less than 15) samples, I recommend you look into confidence intervals based on the
student-t distribution.¬†<a href="#fnref:fnote4" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:fnote4:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:fnotez" role="doc-endnote">
      <p>In doing bootstrapping, we‚Äôre really trying to find the distribution of our
statistic \(\hat S\). So, what we find via this method are bounds \((l,u)\) such that
\(P(l\leq \hat S \leq u)\geq C\). How does this relate to the definition of a
confidence interval? This is a somewhat theoretic exercise, but can be helpful in
clarifying your understanding of the more technical aspects of confidence interval
computation.¬†<a href="#fnref:fnotez" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fnoted" role="doc-endnote">
      <p>Why are you using MATLAB? Stop that. You‚Äôre not in school anymore.¬†<a href="#fnref:fnoted" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fnotec" role="doc-endnote">
      <p>Some of the issues that arise here (for example, over- and
under-fitting) have solutions that are more practical and less theoretical and
statistical in nature - these will be covered in more depth in the machine
learning portion of this guide, and so we don‚Äôt go into too much detail in this
section.¬†<a href="#fnref:fnotec" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fnoteb" role="doc-endnote">
      <p>\(\beta_0\) just represents the difference in the mean of the two
variables, so it could be non-zero even if the two are independent.¬†<a href="#fnref:fnoteb" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET