I"¨L<p>People often use a Gaussian to approximate distributions of sample means. This is
generally justified by the central limit theorem, which states that the sample mean of
an independent and identically distributed sequence of random variables converges to a
normal random variable in distribution.<sup id="fnref:fnote_clt" role="doc-noteref"><a href="#fn:fnote_clt" class="footnote" rel="footnote">1</a></sup> In hypothesis testing, we might use
this to calculate a \(p\)-value, which then is used to drive decision making.</p>

<p>I‚Äôm going to show that calculating \(p\)-values in this way is actually incorrect, and
leads to results that get <em>less</em> accurate as you collect more data! This has
substantial implications for those who care about the statistical rigor of their A/B
tests, which are often based on Gaussian (normal) approximations.</p>

<h1 id="a-simple-example">A Simple Example</h1>

<p>Let‚Äôs take a very simple example. Let‚Äôs say that the prevailing wisdom is that no more
than 20% of people like rollerskating. You suspect that the number is in fact much
larger, and so you decide to run a statistical test. In this test, you model each person
as a Bernoulli random variable with parameter \(p\). <strong>The null hypothesis \(H_0\) is
that \(p\leq 0.2\)</strong>. You decide to go out and ask 100 people their opinions on
rollerskating.<sup id="fnref:fnote_sample" role="doc-noteref"><a href="#fn:fnote_sample" class="footnote" rel="footnote">2</a></sup></p>

<p>You begin gathering data. Unbeknownst to you, it is <em>in fact</em> the case that a full 80%
of the population enjoys rollerskating. So, as you randomly ask people if they enjoy
rollerskating, you end up getting a lot of ‚Äúyes‚Äù responses. Once you‚Äôve gotten 100
responses, you start analyzing the data.</p>

<p>It turns out that you got 74 ‚Äúyes‚Äù responses, and 26 ‚Äúno‚Äù responses. Since you‚Äôre a
practiced statistician, you know that you can calculate a \(p\)-value by finding the
probability that a binomial random variable with parameter \(p_0=0.2\) would generate a
value \(k\geq74\) with \(n=100\). This probability is just</p>

\[p_\text{exact} = \text{Prob}(k\geq 74) = \sum_{k=74}^{n}{n \choose k} p_0^{k} (1-p_0)^{(n-k)}.\]

<p>However, you know that you can approximate a binomial distribution with a Gaussian of
mean \(\mu=np_0\) and variance \(\sigma^2=np_0(1-p_0)\), so you decide to calculate an
<em>approximate</em> \(p\)-value,</p>

\[p_\text{approx} = \frac{1}{\sqrt{2\pi np_0(1-p_0)}}\int_{k=74}^\infty \exp\left(-\frac{(k-np_0)^2}{2np_0(1-p_0)}\right).\]

<p>However, <strong>this approximation is actually incorrect, and will give you progressively
worse estimates of \(p_\text{exact}\).</strong> Let‚Äôs observe this in action.</p>

<h2 id="python-simulation-of-data">Python Simulation of Data</h2>

<p>We simulate data for values \(n=1\) through \(n=1000\), and compute the corresponding
exact and approximate \(p\)-value. We plot the log of the \(p\) value, since they get
very small very quickly.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span><span class="p">,</span> <span class="n">binom</span>

<span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="n">use</span><span class="p">(</span> <span class="p">[</span><span class="s">'classic'</span><span class="p">,</span> <span class="s">'ggplot'</span><span class="p">])</span>

<span class="n">p_true</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">binom</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_true</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">p0</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">p_vals</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">index</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">),</span> 
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'true p-value'</span><span class="p">,</span> <span class="s">'normal approx. p-value'</span><span class="p">]</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">n0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">):</span>
    <span class="n">normal_dev</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n0</span><span class="o">*</span><span class="n">p0</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p0</span><span class="p">))</span>
    <span class="n">normal_mean</span> <span class="o">=</span> <span class="n">n0</span><span class="o">*</span><span class="n">p0</span>
    <span class="n">k</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="n">n0</span><span class="p">])</span>
    <span class="c1"># the "survival function" is 1 - cdf, which is the p-value in our case
</span>    <span class="n">normal_logpval</span> <span class="o">=</span> <span class="n">norm</span><span class="p">.</span><span class="n">logsf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">normal_mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">normal_dev</span><span class="p">)</span>
    <span class="n">true_logpval</span> <span class="o">=</span> <span class="n">binom</span><span class="p">.</span><span class="n">logsf</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p0</span><span class="p">)</span>
    <span class="n">p_vals</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">n0</span><span class="p">,</span> <span class="s">'true p-value'</span><span class="p">]</span> <span class="o">=</span> <span class="n">true_logpval</span>
    <span class="n">p_vals</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">n0</span><span class="p">,</span> <span class="s">'normal approx. p-value'</span><span class="p">]</span> <span class="o">=</span> <span class="n">normal_logpval</span>
    
<span class="n">p_vals</span><span class="p">.</span><span class="n">replace</span><span class="p">([</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">).</span><span class="n">dropna</span><span class="p">().</span><span class="n">plot</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">));</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Number of Samples"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Log-p Value"</span><span class="p">);</span>
</code></pre></div></div>

<p>We have to drop <code class="language-plaintext highlighter-rouge">inf</code>s because after about \(n=850\) or so, the \(p\)-value actually
gets too small for <code class="language-plaintext highlighter-rouge">scipy.stats</code> to calculate; it just returns <code class="language-plaintext highlighter-rouge">-np.inf</code>.</p>

<p>The resulting plot tells a shocking tale:</p>

<p><img src="/assets/images/p-values.png" alt="P-value Divergence" /></p>

<p>The approximation diverges from the exact value! Seeing this, you begin to weep
bitterly. Is the Central Limit Theorem invalid? Has your whole life been a lie? It turns
out that the answer to the first is a resounding no, and the second‚Ä¶ probably also
no. But then what is going on here?</p>

<h2 id="convergence-is-not-enough">Convergence Is Not Enough</h2>

<p>The first thing to note is that, mathematically speaking, the two \(p\)-values
\(p_\text{exact}\) and \(p_\text{approx}\) <strong>do, in fact, converge</strong>. That is to say,
as we increase the number of samples, their difference is approaching zero:</p>

\[\left| p_\text{exact} - p_\text{approx}\right| \rightarrow 0\]

<p>What I‚Äôm arguing, then, is that <strong>convergence is not enough</strong>.</p>

<p>If it were, then we could just approximate the true \(p\)-value with 0. That is, we
could report a \(p\)-value of \(p_\text{approx} = 0\), and claim that since our
approximation is converging to the actual value, it should be taken
seriously. Obviously, this should not be taken seriously as an approximation.</p>

<p>Our intuitive sense of ‚Äúconvergence‚Äù, the sense that \(p_\text{approx}\) is becoming ‚Äúa
better and better approximation of‚Äù \(p_\text{exact}\) as we take more samples,
corresponds to the <em>percent error</em> converging to zero:</p>

\[\left| \frac{p_\text{approx} - p_\text{exact}}{p_\text{exact}}\right| \rightarrow 0.\]

<p>In terms of asymptotic decay, this is a stronger claim than convergence. Rather than
their difference converging to zero, which means it is \(o(1)\), we demand that their
difference converge to zero <em>faster than \(p_\text{exact}\)</em>,</p>

\[\left| p_\text{exact} - p_\text{approx}\right|  = o\left(p_\text{exact}\right).\]

<p>It would also suffice to have an upper bound on the \(p\)-value; that is, if we could
say that \(p_\text{exact} &lt; p_\text{approx}\), so \(p_\text{exact}\) is <em>at worst</em> our
approximate value \(p_\text{approx}\), and we knew that this held regardless of sample
size, then we could report our approximate result knowing that it was at worst a bit
conservative. However, as far as I can see, the central limit theorem and other similar
convergence results give us no such guarantee.</p>

<h2 id="implications">Implications</h2>

<p>What I‚Äôve shown is that for the simple case above, Gaussian approximation is not a
strategy that will get you good estimates of the true \(p\)-value, especially for large
amounts of data. You will under-estimate your \(p\)-value, and therefore overestimate
the strength of evidence you have against the null hypothesis.</p>

<p>Although A/B testing is a slightly more complex scenario, I suspect that the same
problem exists in that realm. A refresher on a typical A/B test scenario: you, as the
administrator of the test, care about the difference between two sample means. If they
samples are from Bernoulli random variables (a good model of click-through rates), then
the <em>true</em> distribution of this difference is the distribution of the difference of
(scaled) binomial random variables, which is more difficult to write down and work
with. Of course, the Gaussian approximation is simple, since the difference of two
Gaussians is again a Gaussian.<sup id="fnref:fnote_AB" role="doc-noteref"><a href="#fn:fnote_AB" class="footnote" rel="footnote">3</a></sup></p>

<p>Most statistical tests are approximate in this way. For example, the \(\chi^2\) test for
goodness of fit is an approximate test. So what are we to make of the fact that this
approximation does not guarantee increasingly valid \(p\)-values? Honestly, I don‚Äôt
know. I‚Äôm sure that others have considered this issue, but I‚Äôm not familiar with the
thinking of the statistical community on it. (As always, please comment if you know
something that would help me understand this better.) All I know is that when doing
tests like this in the future, I‚Äôll be much more careful about how I report my results.</p>

<h1 id="afterword-technical-details">Afterword: Technical Details</h1>

<p>As I said above, the two \(p\)-values do, in fact, converge. However, there is an
interesting mathematical twist in that <strong>the convergence is not guaranteed by the
central limit theorem.</strong> It‚Äôs a bit besides the point, and quite technical, but I found
it so interesting that I thought I should write it up.</p>

<p>As I said, this section isn‚Äôt essential to my central argument about the insufficiency
of simple convergence; it‚Äôs more of an interesting aside.</p>

<h2 id="limitations-of-the-central-limit-theorem">Limitations of the Central Limit Theorem</h2>

<p>To understand the problem, we have to do a deep dive into the details of the central
limit theorem. This will get technical. The TL;DR is that since our \(p\)-values are
getting smaller, the CLT doesn‚Äôt actually guarantee that they will converge.</p>

<p>Suppose we have a sequence of random variables \(X_1, X_2, X_3, \ldots\). These would
be, in the example above, the Bernoulli random variables that represent individual people‚Äôs
responses to your question about rollerskates. Suppose that these random variables are
independent and identically distributed, with mean \(\mu\) and finite variance
\(\sigma^2\).<sup id="fnref:fnote_bin" role="doc-noteref"><a href="#fn:fnote_bin" class="footnote" rel="footnote">4</a></sup></p>

<p>Let \(S_n\) be the sample mean of all the \(X_i\) up through \(n\):</p>

\[S_n = \frac{1}{n} \sum_{i=1}^n X_i.\]

<p>We want to say what distribution the sample mean converges to. First, we know it‚Äôll
converge to something close to the mean, so let‚Äôs subtract that off so that it converges
to something close to zero. So now we‚Äôre considering \(S_n - \mu\). But we also know
that the standard deviation goes down like \(1/\sqrt{n}\), so to get it to converge to
something stable, we have to multiply by \(\sqrt{n}\). So now we‚Äôre considering the
shifted and scaled sample mean \(\sqrt{n}\left(S_n - \mu\right)\).</p>

<p>The central limit theorem states that this converges <strong>in distribution</strong> to a normal
random variable with distribution \(N(0, \sigma^2)\). Notationally, you might see
mathematicians write</p>

\[\sqrt{n}\left(S_n-\mu\right)\ \xrightarrow{D} N(0,\sigma^2).\]

<p>What does it mean that they converge <strong>in distribution</strong>? It means that, for a fixed
area, the areas under the respective curves converge. Note that <strong>we have to fix the
area</strong> to get convergence. Let‚Äôs look at some pictures. First, note that we can plot the
exact distribution of the variable \(\sqrt{n}(S_n-\mu)\); it‚Äôs just a binomial random
variable, appropriately shifted and scaled. We‚Äôll plot this alongside the normal
approximation \(N(0,\sigma^2)\).</p>

<!-- I'd like to have this centered. -->
<p><img src="/assets/images/clt.gif" alt="CLT gif" /></p>

<p>The area under the shaded part of the normal converges to the area of the bars in that
same shaded region. This is what convergence in distribution means.</p>

<p>Now for the crux. As we gather data, it becomes more and more obvious that our null
hypothesis is incorrect - that is, we move further and further out into the tail of the
null hypothesis‚Äô distribution for \(S_n\). This is very intuitive - as we gather more
data, we expect our \(p\)-value to go down. The \(p\)-value is a tail integral of the
distribution, so we expect to be moving further and further into the tail of the
distribution.</p>

<p>Here‚Äôs a gif, where the shaded region represents the \(p\)-value that we‚Äôre calculating:</p>

<!-- I'd like to have this centered. -->
<p><img src="/assets/images/p-val.gif" alt="p-value gif" /></p>

<p>As we increase \(n\), the area we‚Äôre integrating changes. So we don‚Äôt get convergence
guarantees from the CLT.</p>

<h2 id="the-berry-esseen-theorem">The Berry-Esseen Theorem</h2>

<p>It‚Äôs worth noting that there is a stronger statement of convergence that applies
specifically to the convergence of the binomial distribution to the corresponding
Gaussian. It is called the <strong>Barry-Esseen Theorem</strong>, and it states that the maximum
distance between the cumulative probability functions of the binomial and the
corresponding Gaussian is \(o(n^{-1/2})\). This claim, which is akin to uniform
convergence of functions (compare to the pointwise convergence of the CLT) does, in
fact, guarantee that our \(p\)-values will converge.</p>

<p>But, as I‚Äôve said above, this is immaterial, albeit interesting; we know already that
the \(p\)-values converge, and we also know that this is not enough for us to be
reporting one as an approximation of the other.</p>

<!-------------------------------- FOOTER ---------------------------->

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:fnote_clt" role="doc-endnote">
      <p>So long as the variance of the distribution being sampled is finite.¬†<a href="#fnref:fnote_clt" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fnote_sample" role="doc-endnote">
      <p>You should decide this number based on some alternative hypothesis and
a power analysis. Also, you should ensure that you are sampling people evenly -
going to a park, for example, might bias your sample towards those that enjoy
rollerskating.¬†<a href="#fnref:fnote_sample" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fnote_AB" role="doc-endnote">
      <p>I haven‚Äôt done a numerical test on this scenario because the true
distribution (the difference between two scaled binomials) is nontrivial to
calcualte, and numerical issues arise as we calculate such small \(p\)-values, which
SciPy takes care of for us in the above example. But as I said, I would be
unsurprised if our Gaussian-approximated \(p\)-values are increasingly poor
approximations of the true \(p\)-value as we gather more samples.¬†<a href="#fnref:fnote_AB" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fnote_bin" role="doc-endnote">
      <p>In our case, for a single Bernoulli random variable with parameter \(p\),
we have \(\mu=p\) and \(\sigma^2=p(1-p)\).¬†<a href="#fnref:fnote_bin" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET